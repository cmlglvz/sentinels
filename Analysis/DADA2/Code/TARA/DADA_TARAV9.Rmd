---
title: "DADA_TARAV9"
author: "Pato"
date: '2022-06-06'
output: html_document
---
## Libraries

```{r echo=TRUE, message=FALSE, warning=FALSE, include=FALSE}
library(tidyverse)
library(BiocGenerics)
library(ShortRead)
library(dada2)
library(Biostrings)
library(DECIPHER)
```

## Import sequences for quality profile analyzes

Define the following path variable so that it points to the extracted directory on your machine. Then we read in the names of the fastq files, and perform some string manipulation to get matched lists of the forward and reverse fastq files.

```{r, echo=TRUE, message=FALSE, warning=FALSE}
path <- "X:/Documents/Proyectos_R/Global/TARA/TARA_Oceans/"
fns <- list.files(path)
fastqs <- fns[grepl(".fastq.gz", fns)] 
fastqs <- sort(fastqs)
fnFs <- fastqs[grepl("_R1_18S_V9.fastq.gz", fastqs)] # Fws
fnRs <- fastqs[grepl("_R2_18S_V9.fastq.gz", fastqs)] # Rvs

# Get sample names from the first part of the forward read filenames
sample.names <- sapply(strsplit(basename(fnFs), "_"), `[`, 1)

# Para especificar de manera concreta donde se encuentras las secuencias Fw y Rv
fnFs <- paste0(path, fnFs)
fnRs <- paste0(path, fnRs)
```

## Quality profiles visualization
```{r}
plotQualityProfile(fnFs)
plotQualityProfile(fnRs)
```

## Sequence filter and trimming
```{r}
ptm <- proc.time()
filtpth <- file.path(path, "Filtered_")
filtFs <- paste0(filtpth, sample.names, "_F_filt.fastq.gz")
filtRs <- paste0(filtpth, sample.names, "_R_filt.fastq.gz")

out <- filterAndTrim(fnFs, filtFs, fnRs, filtRs, 
                     truncLen=c(120,120), trimLeft=c(10,10), 
                     maxN=0, maxEE=c(2,2), truncQ=2, 
                     compress=TRUE, multithread=FALSE, verbose=TRUE)

rinout <- as.data.frame(out)
write.csv2(rinout, file = "X:/Documents/Proyectos_R/Global/TARA/reads_out.csv")

proc.time() - ptm
```

```{r, echo=TRUE, message=FALSE, warning=FALSE}
filt <- "X:/Documents/Proyectos_R/Global/TARA/Filtered_Camilo/"
filtns <- list.files(filt)
filtqs <- fns[grepl(".fastq.gz", fns)] 
filtqs <- sort(filtqs)
filtFs <- filtqs[grepl("_F_filt.fastq.gz", filtqs)] # Fws
filtRs <- filtqs[grepl("_R_filt.fastq.gz", filtqs)] # Rvs

# Get sample names from the first part of the forward read filenames
filtnames <- sapply(strsplit(basename(filtFs), "_"), `[`, 1)

# Para especificar de manera concreta donde se encuentras las secuencias Fw y Rv
filtFs <- paste0(path, filtFs)
filtRs <- paste0(path, filtRs)
```

## Dereplicate
```{r}
ptm <- proc.time()
derepFs <- derepFastq(filtFs, verbose = TRUE)
derepRs <- derepFastq(filtRs, verbose = TRUE)
names(derepFs) <- sample.names 
names(derepRs) <- sample.names
proc.time() - ptm
```

## Error rate estimation
```{r}
ptm <- proc.time()
errF <- learnErrors(derepFs, multithread = TRUE, randomize = TRUE, verbose = TRUE)
errR <- learnErrors(derepRs, multithread = TRUE, randomize = TRUE, verbose = TRUE)
proc.time() - ptm
```

## Error rate graphs
```{r}
plotErrors(errF, nominalQ = TRUE)
plotErrors(errR, nominalQ = TRUE)
```

##Joint sample inference and error rate estimation
```{r}
ptm <- proc.time()
dadaFs <- dada(derepFs, err = errF, multithread = TRUE, verbose = 1)
dadaRs <- dada(derepRs, err = errR, multithread = TRUE, verbose = 1)
proc.time() - ptm
```

## Merge paired reads
```{r}
ptm <- proc.time()
mergers <- mergePairs(dadaFs, derepFs, dadaRs, derepRs, verbose = TRUE)
class(mergers) #list
length(mergers) #N° of elements in the list, one for each sample
names(mergers)
proc.time() - ptm
#head(mergers[[1]])
```

## Construct ASV table
```{r}
seqtab <- makeSequenceTable(mergers)
dim(seqtab)
table(nchar(getSequences(seqtab))) # Inspect distribution of sequence lengths
```

## Remove chimeras
```{r}
seqtab.nochim <- removeBimeraDenovo(seqtab, method = "consensus", multithread = TRUE, verbose = TRUE) # Por defecto el método es por defecto, revisar en ayuda (F1) de esta función para más información
dim(seqtab.nochim)
sum(seqtab.nochim)/sum(seqtab) #how much have been lost in the process
write.csv2(seqtab.nochim, "X:/Documents/Proyectos_R/Global/TARA/seqtab_nochim_complete.csv")
```

## Track reads through the pipeline
```{r}
getN <- function(x) sum(getUniques(x))
summary_tab <- data.frame(row.names = )
track <- cbind(out, sapply(dadaFs, getN), sapply(dadaRs, getN), sapply(mergers, getN), rowSums(seqtab.nochim))
# If processing a single sample, remove the sapply calls: e.g. replace sapply(dadaFs, getN) with getN(dadaFs)
colnames(track) <- c("input", "filtered", "denoisedF", "denoisedR", "merged", "nochim")
rownames(track) <- sample.names
print(track)
trackdf <- as.data.frame(track)
write.csv2(trackdf, file = "X:/Documents/Proyectos_R/Global/TARA/process_track.csv")

#Alt
summary_tab <- data.frame(row.names = sample.names, 
                          dada2_input = out[, 1], 
                          filtered = out[, 2], 
                          dada_Fw = sapply(dadaFs, getN), 
                          dada_Rv = sapply(dadaRs, getN),
                          merged = sapply(mergers, getN),
                          nonchim = rowSums(seqtab.nochim),
                          final_perc_reads_retained = round(rowSums(seqtab.nochim)/out[, 1]*100, 1)
                          )
write.csv2(summary_tab, file = "X:/Documents/Proyectos_R/Global/TARA/summary_tab_dada2.csv")
#If error in some point, go back to those and roll again
```

## Remove sequence variants seen less than a given number of times
```{r}
seqtab.nochim = seqtab.nochim[,colSums(seqtab.nochim) > 10]
# Opcional, de acuerdo a tus criterios y teniendo en cuenta el impacto que tendrán en tus resultados (el porqué)
## Write sequence table to file
write.csv2(seqtab.nochim, file = "seqtab_nonchim.csv")
```


```{r, echo=TRUE, message=FALSE, warning=FALSE}
seqtab.nochim <- read.csv("seqtab_nonchim.csv", 
                 header = TRUE, 
                 sep = ";", 
                 skip = 0, 
                 row.names = 1)
                 
seqtab.nochim <- as.matrix(seqtab.nochim)
```

## Taxonomy assignation (Silva, RDP, Greengenes, etc.)

                 
```{r Taxonomy assignation, echo=TRUE, message=FALSE, warning=FALSE, include=FALSE}
taxa <- assignTaxonomy(seqtab.nochim, 
                       "X:/Documents/Proyectos_R/IDTAXA/data/pr2_version_5.0.0_SSU_dada2.fasta.gz", 
                       taxLevels = c("Kingdom", 
                                     "Supergroup", 
                                     "Division", 
                                     "Subdivision", 
                                     "Class", 
                                     "Order", 
                                     "Family", 
                                     "Genus", 
                                     "Species"), 
                       multithread = TRUE, 
                       verbose = TRUE)
```


```{r Inspection, echo=TRUE,message=FALSE, warning=FALSE, include=TRUE}
# Removing sequence rownames for display only
strict <- taxa
rownames(strict) <- NULL
head(strict)
```


## Write taxonomy assignments to file (PR2)

### PR2 4.14.0
```{r Taxonomy table, message=FALSE, warning=FALSE, include=TRUE}
write.csv2(taxa, file = "X:/Documents/Proyectos_R/Global/TARA/taxa.csv")
```


###Extracting the goods(to differentiate from usual method goods)
```{r Goods, echo=TRUE, message=TRUE, warning=TRUE, include=TRUE}
#giving our seq headers more manageable names (ASV_1, ASV_2...)
asv_seqs <- colnames(seqtab.nochim)
asv_headers <- vector(dim(seqtab.nochim)[2], mode = "character")

for (i in 1:dim(seqtab.nochim)[2]) {
  asv_headers[i] <- paste(">ASV", i, sep = "_")
}

#making and writing out a fasta of our final ASV seqs:
asv_fasta <- c(rbind(asv_headers, asv_seqs))
write(asv_fasta, "X:/Documents/Proyectos_R/Global/TARA/tara_v9.fasta") #Will be send to dropbox, not sure if push to dataset repository
```

```{r Goods, echo=TRUE, message=TRUE, warning=TRUE, include=TRUE}
#count table
asv_tab <- t(seqtab.nochim) %>% as.data.frame()
rownames(asv_tab) <- sub(">", "", asv_headers)
colnames(asv_tab) <- sample.names
asv_tab <- mutate(asv_tab, Seqs = colnames(seqtab.nochim), .before = "TARA004")
write.csv2(asv_tab, file = "X:/Documents/Proyectos_R/Global/TARA/ASVs_counts_tara_v9.csv")
```



### END