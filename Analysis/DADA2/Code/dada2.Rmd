---
title: "dada2"
date: July - 2023
output: html_document
---

## Libraries
```{r echo=TRUE, message=FALSE, warning=FALSE, include=FALSE}
library(tidyverse)
library(BiocGenerics)
library(ShortRead)
library(dada2)
library(Biostrings)
library(DECIPHER)
```


## Import sequences for quality profile analyzes
Define the following path variable so that it points to the extracted directory on your machine. Then we read in the names of the fastq files, and perform some string manipulation to get matched lists of the forward and reverse fastq files.
```{r, echo=TRUE, message=FALSE, warning=FALSE}
path <- "X:/Documents/Proyectos_R/sentinels/Analysis/DADA2/Sequences/"
fns <- list.files(path)
fastqs <- fns[grepl(".fastq.gz", fns)] 
fastqs <- sort(fastqs)
fnFs <- fastqs[grepl("_L001_R1.fastq.gz", fastqs)] # Fws
fnRs <- fastqs[grepl("_L001_R2.fastq.gz", fastqs)] # Rvs

# Get sample names from the first part of the forward read filenames
sample.names <- sapply(strsplit(basename(fnFs), "_"), `[`, 1)

# Para especificar de manera concreta donde se encuentras las secuencias Fw y Rv
fnFs <- paste0(path, fnFs)
fnRs <- paste0(path, fnRs)
```

Finally, we visualize the quality profiles of the reads:
In **gray-scale** is a heat map of the frequency of each quality score at each base position. The mean quality score at each position is shown by the **green line**, and the quartiles of the quality score distribution by the **orange lines**. The **red line** shows the scaled proportion of reads that extend to at least that position (this is more useful for other sequencing technologies, as Illumina reads are typically all the same lenghth, hence the flat red line).

#### Quality profiles visualization
```{r, echo=TRUE, message=FALSE, warning=FALSE}
plotQualityProfile(fnFs)
plotQualityProfile(fnRs)
```

The **forward reads** are generally of good quality. It is advise to trim the last few nucleotides to avoid less well-controlled errors that can arise there.

The **reverse reads** are of significantly worse quality, especially at the end, which is common in Illumina sequencing. This isn’t too worrisome, as **DADA2** incorporates quality information into its error model which makes the algorithm robust to lower quality sequence, but trimming as the average qualities (for all sequences) crash will improve the algorithm’s sensitivity to rare sequence variants.


## Sequence filter and trimming

Based on quality profiles we can trim **NTs** from left (*trimLeft*) and anything below **Q30** (*truncLen*) on the right. Also we can set filtering parameters according to our needs (computing power, strictness and so on).

```{r, echo=TRUE, message=TRUE, warning=TRUE}
ptm <- proc.time()
filtpth <- file.path(path, "Filtered_")
filtFs <- paste0(filtpth, sample.names, "_F_filt.fastq.gz")
filtRs <- paste0(filtpth, sample.names, "_R_filt.fastq.gz")

out <- filterAndTrim(fnFs, filtFs, fnRs, filtRs, 
                     truncLen=c(250,230), trimLeft=c(10,10), 
                     maxN=0, maxEE=c(2,2), truncQ=2, 
                     compress=TRUE, multithread=FALSE, verbose=TRUE)

rinout <- as.data.frame(out)
write.csv2(rinout, file = "X:/Documents/Proyectos_R/sentinels/Analysis/DADA2/Products/filter_trim_out.csv")

proc.time() - ptm
```

```{r}
out2 <- filterAndTrim(fnFs, filtFs, fnRs, filtRs, 
                     truncLen=c(250,220), trimLeft=c(10,10), 
                     maxN=0, maxEE=c(2,2), truncQ=2, 
                     compress=TRUE, multithread=FALSE, verbose=TRUE)

rinout2 <- as.data.frame(out2)
write.csv2(rinout2, file = "X:/Documents/Proyectos_R/sentinels/Analysis/DADA2/Products/filter_trim_out2.csv")

proc.time() - ptm
```

```{r}
ptm <- proc.time()
filtFs3 <- paste0(filtpth, sample.names, "_F_filt.fastq.gz")
filtRs3 <- paste0(filtpth, sample.names, "_R_filt.fastq.gz")

out3 <- filterAndTrim(fnFs, filtFs3, fnRs, filtRs3, 
                     truncLen=c(250,210), trimLeft=c(10,10), 
                     maxN=0, maxEE=c(2,2), truncQ=2, 
                     compress=TRUE, multithread=FALSE, verbose=TRUE)

rinout3 <- as.data.frame(out3)
write.csv2(rinout3, file = "X:/Documents/Proyectos_R/sentinels/Analysis/DADA2/Products/filter_trim_out_3.csv")

proc.time() - ptm
```

```{r}
ptm <- proc.time()
filtFs4 <- paste0(filtpth, sample.names, "_F_filt.fastq.gz")
filtRs4 <- paste0(filtpth, sample.names, "_R_filt.fastq.gz")

out4 <- filterAndTrim(fnFs, filtFs4, fnRs, filtRs4, 
                     truncLen=c(240,230), trimLeft=c(10,10), 
                     maxN=0, maxEE=c(2,2), truncQ=2, 
                     compress=TRUE, multithread=FALSE, verbose=TRUE)

rinout4 <- as.data.frame(out4)
write.csv2(rinout4, file = "X:/Documents/Proyectos_R/sentinels/Analysis/DADA2/Products/filter_trim_out_4.csv")

proc.time() - ptm
```


The *filterAndTrim(...)* function filters the forward and reverse reads jointly, outputting only those pairs of reads that both pass the filter. In this function call we did four things: We removed the first *trimLeft=10* nucleotides of each read. We truncated the forward and reverse reads at *truncLen=c(240, 200)* nucleotides respectively. We filtered out all reads with more than *maxN=0* ambiguous nucleotides. And we filtered out all reads with more than two expected errors. The filtered output files were stored as gzipped fastq files *(compress=TRUE)*.

This represents a fairly standard set of filtering/trimming parameters. However, it is always worth evaluating whether the filtering and trimming parameters you are using are appropriate for your data. One size does not fit all! (And are you sure you have removed your primers?)

An important consideration: If using paired-end sequencing data, you must maintain a suitable overlap (>20nts) between the forward and reverse reads after trimming! This is especially important to keep in mind for mult-V-region amplicions (such as V3-V4) in which there may be relatively little overlap to begin with, and thus little read-truncation is possible if reads are to be merged later on.


## Dereplicate

The next thing we want to do is “dereplicate” the filtered fastq files. Finding the set of unique sequences, equivalently, the process of finding duplicated (replicate) sequences. Dereplication combines all identical sequencing reads into into “unique sequences” with a corresponding “abundance” equal to the number of reads with that unique sequence. During dereplication, we condense the data by collapsing together all reads that encode the same sequence, which significantly reduces later computation times.

Dereplication in the DADA2 pipeline has one crucial addition from other pipelines: DADA2 retains a summary of the quality information associated with each unique sequence. The consensus quality profile of a unique sequence is the average of the positional qualities from the dereplicated reads. These quality profiles inform the error model of the subsequent sample inference step, significantly increasing DADA2’s accuracy.

```{r,echo=TRUE, message=FALSE, warning=FALSE}
ptm <- proc.time()
derepFs <- derepFastq(filtFs, verbose=TRUE)
derepRs <- derepFastq(filtRs, verbose=TRUE)
names(derepFs) <- sample.names 
names(derepRs) <- sample.names
proc.time() - ptm
```

```{r}
derepFs2 <- derepFastq(filtFs, verbose=TRUE)
derepRs2 <- derepFastq(filtRs, verbose=TRUE)
names(derepFs2) <- sample.names 
names(derepRs2) <- sample.names
proc.time() - ptm
```

```{r}
ptm <- proc.time()
derepFs3 <- derepFastq(filtFs3, verbose=TRUE)
derepRs3 <- derepFastq(filtRs3, verbose=TRUE)
names(derepFs3) <- sample.names 
names(derepRs3) <- sample.names
proc.time() - ptm
```


## Error rate estimation

The DADA2 algorithm makes use of a parametric error model *(err)* and every amplicon dataset has a different set of error rates. The $learnErrors$ method learns the error model from the data, by alternating estimation of the error rates and inference of sample composition until they converge on a jointly consistent solution. As in many machine-learning problems, the algorithm must begin with an initial guess, for which the maximum possible error rates in this data are used (the error rates if only the most abundant sequence is correct and all the rest are errors).

```{r, echo=TRUE, message=FALSE, warning=FALSE}
ptm <- proc.time()
errF <- learnErrors(filtFs, multithread=TRUE, randomize = TRUE)
errR <- learnErrors(filtRs, multithread=TRUE, randomize = TRUE)
proc.time() - ptm
```

```{r}
errF2 <- learnErrors(derepFs2, multithread=TRUE, randomize = TRUE)
errR2 <- learnErrors(derepRs2, multithread=TRUE, randomize = TRUE)
proc.time() - ptm
```

```{r}
ptm <- proc.time()
errF3 <- learnErrors(derepFs3, multithread=TRUE, randomize = TRUE)
errR3 <- learnErrors(derepRs3, multithread=TRUE, randomize = TRUE)
proc.time() - ptm
```


It is always worthwhile to visualize the estimated error rates:

#### Error rate graphs.

```{r, echo=TRUE, message=FALSE, warning=FALSE}
plotErrors(errF, nominalQ=TRUE)
plotErrors(errR, nominalQ=TRUE)
```
The error rates for each possible transition (A→C, A→G, …) are shown. **Points** are the observed error rates for each consensus quality score. The **black line** shows the estimated error rates after convergence of the machine-learning algorithm. The **red line** shows the error rates expected under the nominal definition of the Q-score.


## Joint sample inference and error rate estimation.

```{r, echo=TRUE, message=FALSE, warning=FALSE}
ptm <- proc.time()
dadaFs <- dada(derepFs, err=errF, multithread=TRUE)
dadaRs <- dada(derepRs, err=errR, multithread=TRUE)
proc.time() - ptm
```

By default, the dada function processes each sample independently. However, pooling information across samples can increase sensitivity to sequence variants that may be present at very low frequencies in multiple samples. The dada2 package offers two types of pooling. *dada(..., pool=TRUE)* performs standard pooled processing, in which all samples are pooled together for sample inference. *dada(..., pool="pseudo")* performs pseudo-pooling, in which samples are processed independently after sharing information between samples, approximating pooled sample inference in linear time.


## Merge paired reads.

We now merge the forward and reverse reads together to obtain the full denoised sequences. Merging is performed by aligning the denoised forward reads with the reverse-complement of the corresponding denoised reverse reads, and then constructing the merged “contig” sequences. By default, merged sequences are only output if the forward and reverse reads overlap by at least 12 bases, and are identical to each other in the overlap region (these conditions can be changed via function arguments). Non-overlapping reads are supported, but not recommended

```{r, echo=TRUE, message=FALSE, warning=FALSE}
mergers <- mergePairs(dadaFs, derepFs, dadaRs, derepRs, verbose=TRUE)
class(mergers) #list
length(mergers) #63 elements in the list, one for each sample
names(mergers)
#head(mergers[[1]])
```

The mergers object is a list of data.frames from each sample. Each data.frame contains the merged *$sequence*, its *$abundance*, and the indices of the *$forward* and *$reverse* sequence variants that were merged. Paired reads that did not exactly overlap were removed by *mergePairs*, further reducing spurious output.


## Construct ASV table

The sequence table is a *matrix* with rows corresponding to (and named by) the samples, and columns corresponding to (and named by) the sequence variants.

```{r, echo=TRUE, message=FALSE, warning=FALSE}
seqtab <- makeSequenceTable(mergers)
dim(seqtab)
table(nchar(getSequences(seqtab))) # Inspect distribution of sequence lengths
```


## Remove chimeras

The core dada method corrects substitution and indel errors, but chimeras remain. Fortunately, the accuracy of sequence variants after denoising makes identifying chimeric ASVs simpler than when dealing with fuzzy OTUs. Chimeric sequences are identified if they can be exactly reconstructed by combining a left-segment and a right-segment from two more abundant “parent” sequences.

The frequency of chimeric sequences varies substantially from dataset to dataset, and depends on factors including experimental procedures and sample complexity.

Most of your **reads** should remain after chimera removal (it is not uncommon for a majority of **sequence variants** to be removed though). If most of your reads were removed as chimeric, upstream processing may need to be revisited. In almost all cases this is caused by primer sequences with ambiguous nucleotides that were not removed prior to beginning the DADA2 pipeline.

By default the *method* is set in **"consensus"** where the samples in a sequence table are independently checked for bimeras, and a **consensus decision on each sequence variant is made**. If it sets as **"per-sample"**, samples in a sequence table are independently checked for bimeras, **and sequence variants are removed (zeroed-out) from samples independently**. Another alternative is **"pooled"** the samples fot bimera identification.

```{r, echo=TRUE, message=FALSE, warning=FALSE}
seqtab.nochim <- removeBimeraDenovo(seqtab, method = "consensus", verbose = TRUE) # Por defecto el método es por defecto, revisar en ayuda (F1) de esta función para más información
dim(seqtab.nochim)
sum(seqtab.nochim)/sum(seqtab) #how much have been lost in the process
```


## Track reads through the pipeline

As a final check of our progress, we’ll look at the number of reads that made it through each step in the pipeline:

```{r, echo=TRUE, message=FALSE, warning=FALSE}
getN <- function(x) sum(getUniques(x))
summary_tab <- data.frame(row.names = )
track <- cbind(out, sapply(dadaFs, getN), sapply(dadaRs, getN), sapply(mergers, getN), rowSums(seqtab.nochim))
# If processing a single sample, remove the sapply calls: e.g. replace sapply(dadaFs, getN) with getN(dadaFs)
colnames(track) <- c("input", "filtered", "denoisedF", "denoisedR", "merged", "nochim")
rownames(track) <- sample.names
print(track)
trackdf <- as.data.frame(track)
write.csv2(trackdf, file = "process_track.csv")
#Alt
Samples <- c("C1A17", "C1F18", "C1A18", "C2A17", "C2F18", "C2A18", "C3A17", "C3F18", "C3A18", "C4A17", "C4F18", "C4A18", 
             "F1A17", "F1F18", "F1A18", "F2A17", "F2F18", "F2A18", "F3A17", "F3F18", "F3A18", "F4A17", "F4F18", "F4A18", 
             "H1A17", "H1F18", "H1A18", "H2A17", "H2F18", "H2A18", "H3A17", "H3F18", "H3A18", "H4A17", "H4F18", "H4A18", 
             "P1F18", "P1A18", "P2F18", "P3F18", "P4F18", 
             "Q1A17", "Q1F18", "Q2A17", "Q2F18", "Q3A17", "Q3F18", "Q3A18", "Q4A17", "Q4F18", "Q4A18", 
             "L1A17", "L1F18", "L1A18", "L2A17", "L2F18", "L2A18", "L3A17", "L3F18", "L3A18", "L4A17", "L4F18", "L4A18")
summary_tab <- data.frame(row.names = Samples, 
                          dada2_input = out[, 1], 
                          filtered = out[, 2], 
                          dada_Fw = sapply(dadaFs, getN), 
                          dada_Rv = sapply(dadaRs, getN),
                          merged = sapply(mergers, getN),
                          nonchim = rowSums(seqtab.nochim),
                          final_perc_reads_retained = round(rowSums(seqtab.nochim)/out[, 1]*100, 1)
                          )
write.csv2(summary_tab, file = "summary_tab_dada2.csv")
#If error in some point, go back to those and roll again
```

Outside of filtering, there should no step in which a majority of reads are lost. If a majority of reads failed to merge, you may need to revisit the *truncLen* parameter used in the filtering step and make sure that the truncated reads span your amplicon. If a majority of reads were removed as chimeric, you may need to revisit the removal of primers, as the ambiguous nucleotides in unremoved primers interfere with chimera identification.

#### Remove sequence variants seen less than a given number of times 
```{r, echo=TRUE, message=FALSE, warning=FALSE}
seqtab.nochim = seqtab.nochim[,colSums(seqtab.nochim) > 10]
# Opcional, de acuerdo a tus criterios y teniendo en cuenta el impacto que tendrán en tus resultados (el porqué)
```


## Write sequence table to file
```{r, echo=TRUE, message=FALSE, warning=FALSE}
write.csv2(seqtab.nochim, file = "seqtab_nonchim.csv")
```


## Taxonomy assignation (Silva, RDP, Greengenes, etc.)

It is common at this point, especially in 16S/18S/ITS amplicon sequencing, to assign taxonomy to the sequence variants. The DADA2 package provides a native implementation of the naive Bayesian classifier method for this purpose. The *assignTaxonomy* function takes as input a set of sequences to be classified and a training set of reference sequences with known taxonomy, and outputs taxonomic assignments with at least *minBoot* bootstrap confidence.

```{r Taxonomy assignation, echo=TRUE, message=FALSE, warning=FALSE, include=FALSE}
taxa <- assignTaxonomy(seqtab.nochim, "X:/Documents/Proyectos_R/IDTAXA/data/pr2_version_4.14.0_SSU_dada2.fasta.gz", taxLevels = c("Kingdom", "Supergroup", "Division", "Class", "Order", "Family", "Genus", "Species"), multithread = TRUE)
```


```{r Inspection, echo=TRUE,message=FALSE, warning=FALSE, include=TRUE}
# Removing sequence rownames for display only
strict <- taxa
rownames(strict) <- NULL
head(strict)
```


## Write taxonomy assignments to file (PR2)

### PR2 4.14.0
```{r Taxonomy table, message=FALSE, warning=FALSE, include=TRUE}
write.csv2(taxa, file = "taxa.csv")
```

###Extracting the goods(to differentiate from usual method goods)
```{r Goods, echo=TRUE, message=TRUE, warning=TRUE, include=TRUE}
#giving our seq headers more manageable names (ASV_1, ASV_2...)
asv_seqs <- colnames(seqtab.nochim)
asv_headers <- vector(dim(seqtab.nochim)[2], mode = "character")

for (i in 1:dim(seqtab.nochim)[2]) {
  asv_headers[i] <- paste(">ASV", i, sep = "_")
}

#making and writing out a fasta of our final ASV seqs:
asv_fasta <- c(rbind(asv_headers, asv_seqs))
write(asv_fasta, "secuencias_core.fasta") #Will be send to dropbox, not sure if push to dataset repository

#count table
asv_tab <- t(seqtab.nochim) %>% as.data.frame()
rownames(asv_tab) <- sub(">", "", asv_headers)
colnames(asv_tab) <- c("C1A17", "C1F18", "C1A18", "C2A17", "C2F18", "C2A18", "C3A17", "C3F18", "C3A18", "C4A17", "C4F18", "C4A18", 
                       "F1A17", "F1F18", "F1A18", "F2A17", "F2F18", "F2A18", "F3A17", "F3F18", "F3A18", "F4A17", "F4F18", "F4A18", 
                       "H1A17", "H1F18", "H1A18", "H2A17", "H2F18", "H2A18", "H3A17", "H3F18", "H3A18", "H4A17", "H4F18", "H4A18", 
                       "P1F18", "P1A18", "P2F18", "P3F18", "P4F18", 
                       "Q1A17", "Q1F18", "Q2A17", "Q2F18", "Q3A17", "Q3F18", "Q3A18", "Q4A17", "Q4F18", "Q4A18", 
                       "L1A17", "L1F18", "L1A18", "L2A17", "L2F18", "L2A18", "L3A17", "L3F18", "L3A18", "L4A17", "L4F18", "L4A18")
asv_tab <- mutate(asv_tab, Seqs = all_of(colnames(seqtab.nochim)), .before = "C1A17")
write.csv2(asv_tab, file = "ASVs_counts.csv")
```



### END
